# Knowledge-Distillation

### Tổng quan
  Knowledge Distillation (KD) là một kỹ thuật trong lĩnh vực học máy và trí tuệ nhân tạo, nhằm mục đích chuyển giao kiến thức từ một mô hình lớn (teacher model) sang một mô hình nhỏ hơn (student model)
  Mục tiêu của kỹ thuật này là tạo ra một mô hình nhỏ gọn hơn có khả năng thực hiện các nhiệm vụ tương tự như mô hình lớn, nhưng với hiệu suất tính toán thấp hơn như trong các thiết bị di động hoặc các hệ thống nhúng, tối ưu hóa việc sử dụng tài nguyên và tăng khả năng triển khai mô hình trong các ứng dụng thực tế như xe không người lái, hệ thống giao thông thông minh,...
### Ý tưởng:
